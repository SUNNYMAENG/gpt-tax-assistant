# LangChain + OpenAI ê¸°ë°˜ GPT ì„¸ë¬´ë¹„ì„œ êµ¬ì¡°í™” ìë£Œ ê²€ìƒ‰ ì‘ë‹µ íë¦„ ì˜ˆì‹œ

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.schema import Document
import json

# 1. ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° (jsonl â†’ LangChain ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜)
def load_jsonl(file_path):
    docs = []
    with open(file_path, encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            docs.append(Document(page_content=item["content"], metadata={"source": item["title"]}))
    return docs

# 2. íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ë²•ë ¹/í†µë‹¬/íŒë¡€/ì„œì‹)
law_docs = load_jsonl("ë²•ë ¹_LangChain_docs.jsonl")
tongdal_docs = load_jsonl("í†µë‹¬_LangChain_docs.jsonl")
case_docs = load_jsonl("íŒë¡€_LangChain_docs.jsonl")
form_docs = load_jsonl("ì„œì‹_LangChain_docs.jsonl")

# 3. ì „ì²´ ë¬¸ì„œ í†µí•© í›„ ë²¡í„°í™”
all_docs = law_docs + tongdal_docs + case_docs + form_docs

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
doc_chunks = text_splitter.split_documents(all_docs)

embedding = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(doc_chunks, embedding)

# 4. GPT ëª¨ë¸ ì„¤ì • + ê²€ìƒ‰ QA ì²´ì¸ êµ¬ì„±
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# 5. ì§ˆë¬¸ ì˜ˆì‹œ
query = "çµ¦ä¸æ˜ç´°æ›¸ã«è¨˜è¼‰ã™ã‚‹æ§é™¤é …ç›®ã¯ã©ã®é€šé”ã«åŸºã¥ãã¾ã™ã‹ï¼Ÿ"
result = qa_chain.run(query)

print("\n[ğŸ” ì§ˆë¬¸ ê²°ê³¼]")
print(result)
